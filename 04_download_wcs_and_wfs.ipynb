{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a39183",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57164",
   "metadata": {},
   "source": [
    "# Simplified access to Veluwe ecological data through OGC Web Services\n",
    "\n",
    "**Author:** Hudson Passos  \n",
    "**Internship host:** Netherlands Institute of Ecology (NIOO-KNAW)  \n",
    "**Host supervisor:** Stefan Vriend (NIOO-KNAW)  \n",
    "**WUR supervisor:** Liesbeth Bakker (WUR, NIOO-KNAW)  \n",
    "**Repository:** [research-project-internship-nioo](https://github.com/hudsonpassos/research-project-internship-nioo)  \n",
    "**Date:** July 18, 2025  \n",
    "**Python version:** 3.11.9  \n",
    "**License:** MIT  \n",
    "**Description:**  \n",
    "This notebook is part of a research internship project. It focuses on the automated selection, filtering, \n",
    "and preprocessing of open ecological geospatial datasets for the Veluwe region using OGC Web Services (WCS and WFS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c5972",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ddb90",
   "metadata": {},
   "source": [
    "# **Part 4**: Download via WCS and WFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08636a00",
   "metadata": {},
   "source": [
    "### 1.1. Initialization: packages, paths, and spatial inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6cd02",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fa390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from owslib.csw import CatalogueServiceWeb\n",
    "from owslib.ows import ServiceIdentification\n",
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlencode\n",
    "from xml.etree import ElementTree as ET\n",
    "from shapely.geometry import box\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from rasterio.merge import merge\n",
    "import rasterio\n",
    "import glob\n",
    "import math\n",
    "from pyproj import Transformer\n",
    "from urllib.parse import urlencode\n",
    "import tempfile\n",
    "#import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse, urlunparse, urlencode\n",
    "import subprocess\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1f892",
   "metadata": {},
   "source": [
    "**Setting pathways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C:/Users/hudso/OneDrive/Documents/02. WUR/11. INTERNSHIP\"\n",
    "outlines_path = os.path.join(root, \"02 data/outlines\")\n",
    "output_path = os.path.join(root, \"05 python_project/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88786",
   "metadata": {},
   "source": [
    "**Loading data frames and creating column 'download'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wcs = pd.read_csv(\"checkpoint03_sf2_ngr_WCS_metadata.csv\")\n",
    "df_wfs = pd.read_csv(\"checkpoint04_ngr_WFS_metadata.csv\")\n",
    "\n",
    "df_wcs['download'] = 'no'\n",
    "df_wfs['download'] = 'no'\n",
    "\n",
    "df_wcs.to_csv(\"checkpoint05_ngr_WCS_metadata.csv\", index=False)\n",
    "df_wfs.to_csv(\"checkpoint05_ngr_WFS_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be648b77",
   "metadata": {},
   "source": [
    "**Loading outlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLshp_path = os.path.join(outlines_path, \"netherlands28992_outline.gpkg\")\n",
    "VELshp_path = os.path.join(outlines_path, \"veluwe28992_outline.gpkg\")\n",
    "\n",
    "NLgdf = gpd.read_file(NLshp_path)\n",
    "VELgdf = gpd.read_file(VELshp_path)\n",
    "\n",
    "netherlands4326 = NLgdf.to_crs(epsg=4326)\n",
    "netherlands28992 = NLgdf\n",
    "\n",
    "veluwe4326 = VELgdf.to_crs(epsg=4326)\n",
    "veluwe28992 = VELgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62c5ca",
   "metadata": {},
   "source": [
    "**Function to make the outline square**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square_bounds(gdf, target_epsg=28992):\n",
    "    \"\"\"\n",
    "    Computes a square bounding box centered on the original bounds of a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf: GeoDataFrame\n",
    "    - target_epsg: EPSG code to reproject the GeoDataFrame before computing bounds (default: 28992)\n",
    "\n",
    "    Returns:\n",
    "    - square_bounds: NumPy array [minx, miny, maxx, maxy] of the square bounding box\n",
    "    \"\"\"\n",
    "    # Project to desired CRS\n",
    "    gdf_proj = gdf.to_crs(epsg=target_epsg)\n",
    "\n",
    "    # Original bounds\n",
    "    minx, miny, maxx, maxy = gdf_proj.total_bounds\n",
    "\n",
    "    # Width and height\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "\n",
    "    # Max side length\n",
    "    max_side = max(width, height)\n",
    "\n",
    "    # Center coordinates\n",
    "    center_x = (minx + maxx) / 2\n",
    "    center_y = (miny + maxy) / 2\n",
    "\n",
    "    # Square bounds\n",
    "    half_side = max_side / 2\n",
    "    square_bounds = np.array([\n",
    "        center_x - half_side,\n",
    "        center_y - half_side,\n",
    "        center_x + half_side,\n",
    "        center_y + half_side\n",
    "    ])\n",
    "\n",
    "    return square_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_bounds_VEL = get_square_bounds(VELgdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5debd49",
   "metadata": {},
   "source": [
    "### 1.2. Download WCS:\n",
    "Download entire data **directly** if it is a coarse resolution, or **in tiles** (if finer resolution), followed by merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56371aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cached Axis Label Fetcher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "_axis_cache = {}\n",
    "\n",
    "def get_axis_labels(wcs_base: str, cov_id: str):\n",
    "    \"\"\"\n",
    "    Fetch the canonical axis names for a given coverage from the WCS DescribeCoverage endpoint,\n",
    "    using an in-memory cache to avoid repeated HTTP calls for the same coverage.\n",
    "    \"\"\"\n",
    "    cache_key = (wcs_base, cov_id)\n",
    "    if cache_key in _axis_cache:\n",
    "        return _axis_cache[cache_key]\n",
    "\n",
    "    params = {\n",
    "        \"service\":    \"WCS\",\n",
    "        \"version\":    \"2.0.1\",\n",
    "        \"request\":    \"DescribeCoverage\",\n",
    "        \"coverageId\": cov_id,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(wcs_base.replace(\"/ows\", \"/wcs\"), params=params, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        ns = {\"gml\": \"http://www.opengis.net/gml/3.2\"}\n",
    "        env = root.find(\".//gml:Envelope\", namespaces=ns)\n",
    "        labels = env.attrib[\"axisLabels\"].split()\n",
    "        _axis_cache[cache_key] = (labels[0], labels[1])\n",
    "        return labels[0], labels[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to fetch axis labels for {cov_id}: {e}\")\n",
    "        # fallback to standard 'x', 'y' if failed\n",
    "        return \"x\", \"y\"\n",
    "\n",
    "def get_base_wfs_url(full_url):\n",
    "    parsed = urlparse(full_url)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))\n",
    "\n",
    "def detect_wcs_version(cap_url):\n",
    "    def try_detect(url):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            content = r.content.decode(\"utf-8\")\n",
    "            root = ET.fromstring(content)\n",
    "\n",
    "            # 1. Try version attribute\n",
    "            ver = root.attrib.get(\"version\")\n",
    "            if ver in (\"2.0.1\", \"1.1.1\", \"1.0.0\"):\n",
    "                return ver\n",
    "\n",
    "            # 2. Fallback: search for known namespaces\n",
    "            if \"http://www.opengis.net/wcs/2.0\" in content:\n",
    "                return \"2.0.1\"\n",
    "            if \"http://www.opengis.net/wcs/1.1\" in content:\n",
    "                return \"1.1.1\"\n",
    "            if \"http://www.opengis.net/wcs\" in content:\n",
    "                return \"1.0.0\"\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed WCS version check for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 1. Try full URL first\n",
    "    version = try_detect(cap_url)\n",
    "    if version:\n",
    "        return version\n",
    "\n",
    "    # 2. Fallback: use cleaned base URL and add GetCapabilities\n",
    "    base_url = get_base_wfs_url(cap_url)\n",
    "    fallback_url = f\"{base_url}?service=WCS&request=GetCapabilities\"\n",
    "    version = try_detect(fallback_url)\n",
    "    if version:\n",
    "        return version\n",
    "\n",
    "    # 3. Default fallback\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def split_square_tiles(outline, bbox, resx, resy, max_px=4000):\n",
    "    \"\"\"\n",
    "    Split bbox = [xmin, ymin, xmax, ymax] into square tiles so that\n",
    "    each tile is <= max_px Ã— max_px pixels at resolution resx, resy.\n",
    "    \n",
    "    Only return tiles that intersect the given 'outline' geometry.\n",
    "    'outline' can be a shapely Polygon/MultiPolygon or a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'outline' is a shapely geometry\n",
    "    if hasattr(outline, \"geometry\"):  # It's a GeoDataFrame\n",
    "        outline_geom = outline.unary_union\n",
    "    else:  # It's already a shapely geometry\n",
    "        outline_geom = outline\n",
    "\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    tile_size = min(max_px * resx, max_px * resy)\n",
    "\n",
    "    nx = math.ceil((xmax - xmin) / tile_size)\n",
    "    ny = math.ceil((ymax - ymin) / tile_size)\n",
    "    \n",
    "    tiles = []\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            x0 = xmin + i * tile_size\n",
    "            y0 = ymin + j * tile_size\n",
    "            x1 = min(x0 + tile_size, xmax)\n",
    "            y1 = min(y0 + tile_size, ymax)\n",
    "            \n",
    "            tile_geom = box(x0, y0, x1, y1)\n",
    "            \n",
    "            if tile_geom.intersects(outline_geom):\n",
    "                tiles.append([x0, y0, x1, y1])\n",
    "\n",
    "    return tiles\n",
    "\n",
    "def convert_bbox_to_degrees(bbox):\n",
    "    transformer = Transformer.from_crs(\"EPSG:28992\", \"EPSG:4258\", always_xy=True)\n",
    "    xmin, ymin = transformer.transform(bbox[0], bbox[1])\n",
    "    xmax, ymax = transformer.transform(bbox[2], bbox[3])\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "# â”€â”€ Download function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def download_tile_28992(row, bbox, resx, resy, tile_index):\n",
    "    \"\"\"\n",
    "    Download one EPSG:28992 tile at resolution (resx,resy) passed in.\n",
    "    \"\"\"\n",
    "    base_url = row[\"wcs_getcapabilities_url\"].split(\"?\")[0]\n",
    "    version  = detect_wcs_version(row[\"wcs_getcapabilities_url\"])\n",
    "\n",
    "    if not version:\n",
    "        print(f\"âŒ {row['layer']}: cannot detect WCS version\")\n",
    "        return None\n",
    "\n",
    "    xmin, ymin, xmax, ymax = [round(v, 3) for v in bbox]\n",
    "    width_px  = int((xmax - xmin) / resx)\n",
    "    height_px = int((ymax - ymin) / resy)\n",
    "    if width_px > 4000 or height_px > 4000:\n",
    "        print(f\"âŒ {row['layer']}: {width_px}Ã—{height_px} px > 4000 limit\")\n",
    "        return None\n",
    "\n",
    "    layer    = row[\"layer\"]\n",
    "    out_path = os.path.join(tile_folder, f\"{layer}_tile{tile_index}.tif\")\n",
    "\n",
    "    x_label, y_label = get_axis_labels(base_url, layer)\n",
    "    \n",
    "    # Common params for WCS 2.0.1\n",
    "    common = {\n",
    "        \"service\":       \"WCS\",\n",
    "        \"request\":       \"GetCoverage\",\n",
    "        \"version\":       version,\n",
    "        \"format\":        \"image/tiff\",\n",
    "        \"coverageId\":    layer,\n",
    "        \"subset\":        [f\"{x_label}({xmin},{xmax})\", f\"{y_label}({ymin},{ymax})\"],\n",
    "        \"subsettingcrs\":\"EPSG:28992\",\n",
    "        \"outputcrs\":     \"EPSG:28992\"\n",
    "    }\n",
    "\n",
    "    # WCS 2.0.1: try scaleSize, then resx/resy\n",
    "    if version == \"2.0.1\":\n",
    "        # 1) scaleSize\n",
    "        p = {**common,\n",
    "            \"scaleSize\":   f\"{x_label}({width_px})\",\n",
    "            \"scaleSize_y\": f\"{y_label}({height_px})\"}\n",
    "        print(\"ğŸ”—\", f\"{base_url}?{urlencode(p, doseq=True)}\")\n",
    "        try:\n",
    "            r = requests.get(base_url, params=p, timeout=60); r.raise_for_status()\n",
    "            if \"image\" in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "                open(out_path,\"wb\").write(r.content)\n",
    "                print(f\"âœ… {layer} tile {tile_index} (scaleSize)\")\n",
    "                return out_path\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ scaleSize failed:\", e)\n",
    "\n",
    "        # 2) fallback to resx/resy\n",
    "        p = {**common,\n",
    "             \"resx\": round(resx,5),\n",
    "             \"resy\": round(resy,5)}\n",
    "        print(\"ğŸ”—\", f\"{base_url}?{urlencode(p, doseq=True)}\")\n",
    "        try:\n",
    "            r = requests.get(base_url, params=p, timeout=60); r.raise_for_status()\n",
    "            if \"image\" in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "                open(out_path,\"wb\").write(r.content)\n",
    "                print(f\"âœ… {layer} tile {tile_index} (resx/resy)\")\n",
    "                return out_path\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ resx/resy failed:\", e)\n",
    "\n",
    "    # WCS 1.0.0: BBOX + width/height\n",
    "    else:\n",
    "        p = {\n",
    "            \"service\": \"WCS\", \"request\": \"GetCoverage\",\n",
    "            \"version\": version, \"coverage\": layer,\n",
    "            \"CRS\":    \"EPSG:28992\",\n",
    "            \"BBOX\":   f\"{xmin},{ymin},{xmax},{ymax}\",\n",
    "            \"width\":  width_px, \"height\": height_px,\n",
    "            \"format\": \"image/tiff\"\n",
    "        }\n",
    "        print(\"ğŸ”—\", f\"{base_url}?{urlencode(p, doseq=True)}\")\n",
    "        try:\n",
    "            r = requests.get(base_url, params=p, timeout=60); r.raise_for_status()\n",
    "            if \"image\" in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "                open(out_path,\"wb\").write(r.content)\n",
    "                print(f\"âœ… {layer} tile {tile_index} (WCS1.0)\")\n",
    "                return out_path\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ WCS1.0 failed:\", e)\n",
    "\n",
    "    print(f\"âŒ {layer} tile {tile_index} failed (v{version})\")\n",
    "    return None\n",
    "\n",
    "###### DEGREES\n",
    "\n",
    "def download_tile_degrees(row, bbox_m, native_resolution, tile_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Downloads WCS coverages in geographic CRS (EPSG:4258/4326).\n",
    "    - bbox_m: your EPSG:28992 bbox in meters\n",
    "    - native_resolution: bool\n",
    "    Returns list of saved TIFF file paths.\n",
    "    \"\"\"\n",
    "    from pyproj import Transformer\n",
    "    import ast, os, requests\n",
    "    from urllib.parse import urlencode\n",
    "\n",
    "    # 1) Convert bbox from 28992â†’degrees\n",
    "    tf = Transformer.from_crs(\"EPSG:28992\", f\"EPSG:{row['crs_epsg']}\", always_xy=True)\n",
    "    xmin_d, ymin_d = tf.transform(bbox_m[0], bbox_m[1])\n",
    "    xmax_d, ymax_d = tf.transform(bbox_m[2], bbox_m[3])\n",
    "    bbox_deg = [xmin_d, ymin_d, xmax_d, ymax_d]\n",
    "\n",
    "    # 2) Parse native spacing (in degrees) from CSV\n",
    "    native_resx, native_resy = map(float, ast.literal_eval(row[\"spatial_resolution\"]))\n",
    "\n",
    "    # 3) Decide deg/px and tile grid\n",
    "    if native_resolution:\n",
    "        # true native spacing â†’ split into 4000Ã—4000-px squares\n",
    "        resx, resy = native_resx, native_resy\n",
    "        tiles = split_square_tiles(bbox_deg, resx, resy, max_px=4000)\n",
    "    else:\n",
    "        # downsample whole extent to one tile â‰¤4000 px on longest side\n",
    "        w = xmax_d - xmin_d\n",
    "        h = ymax_d - ymin_d\n",
    "        target_px = 4000\n",
    "        # choose degree/px so that max(w, h)/deg_per_px = â‰¤4000\n",
    "        deg_per_px = max(w, h) / target_px\n",
    "        resx = resy = deg_per_px\n",
    "        tiles = [bbox_deg]\n",
    "\n",
    "    saved = []\n",
    "    base    = row[\"wcs_getcapabilities_url\"].split(\"?\")[0]\n",
    "    version = detect_wcs_version(row[\"wcs_getcapabilities_url\"])\n",
    "    if not version:\n",
    "        print(f\"âŒ {row['layer']}: cannot detect WCS version\")\n",
    "        return saved\n",
    "\n",
    "    coverage = row[\"layer\"]\n",
    "    crs_code = int(row[\"crs_epsg\"])\n",
    "\n",
    "    for idx, tb in enumerate(tiles, start=1):\n",
    "        xmin, ymin, xmax, ymax = [round(v, 9) for v in tb]\n",
    "        wp = int((xmax - xmin) / resx)\n",
    "        hp = int((ymax - ymin) / resy)\n",
    "        if wp > 4000 or hp > 4000:\n",
    "            print(f\"âŒ {coverage}: {wp}Ã—{hp} px > 4000\")\n",
    "            continue\n",
    "\n",
    "        if native_resolution:\n",
    "            out = os.path.join(tile_folder, f\"{coverage}_deg_tile{idx}.tif\")\n",
    "        else:\n",
    "            out = os.path.join(tile_folder, f\"{coverage}.tif\")\n",
    "        \n",
    "        x_label, y_label = get_axis_labels(base, coverage)\n",
    "        \n",
    "        common = {\n",
    "            \"service\":       \"WCS\",\n",
    "            \"request\":       \"GetCoverage\",\n",
    "            \"version\":       version,\n",
    "            \"format\":        \"image/tiff\",\n",
    "            \"coverageId\":    coverage,\n",
    "            \"subset\": [f\"{x_label}({xmin},{xmax})\", f\"{y_label}({ymin},{ymax})\"],\n",
    "            \"subsettingcrs\": f\"EPSG:{crs_code}\",\n",
    "            \"outputcrs\":     f\"EPSG:{crs_code}\"\n",
    "        }\n",
    "\n",
    "        # WCS 2.x\n",
    "        if version.startswith(\"2\"):\n",
    "            # try scaleSize\n",
    "            p = {**common, \"scaleSize\":   f\"{x_label}({wp})\", \"scaleSize_y\": f\"{y_label}({hp})\"}\n",
    "            print(\"ğŸ”—\", f\"{base}?{urlencode(p, doseq=True)}\")\n",
    "            try:\n",
    "                r = requests.get(base, params=p, timeout=60); r.raise_for_status()\n",
    "                if \"image\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                    open(out, \"wb\").write(r.content)\n",
    "                    print(f\"âœ… {coverage} tile{idx} (scaleSize)\")\n",
    "                    saved.append(out)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ scaleSize failed:\", e)\n",
    "\n",
    "            # fallback to resx/resy\n",
    "            p = {**common, \"resx\": round(resx, 9), \"resy\": round(resy, 9)}\n",
    "            print(\"ğŸ”—\", f\"{base}?{urlencode(p, doseq=True)}\")\n",
    "            try:\n",
    "                r = requests.get(base, params=p, timeout=60); r.raise_for_status()\n",
    "                if \"image\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                    open(out, \"wb\").write(r.content)\n",
    "                    print(f\"âœ… {coverage} tile{idx} (resx/resy)\")\n",
    "                    saved.append(out)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ resx/resy failed:\", e)\n",
    "\n",
    "        # WCS 1.0\n",
    "        else:\n",
    "            p = {\n",
    "                \"service\": \"WCS\",\n",
    "                \"request\": \"GetCoverage\",\n",
    "                \"version\": version,\n",
    "                \"coverage\": coverage,\n",
    "                \"CRS\":      f\"EPSG:{crs_code}\",\n",
    "                \"BBOX\":     f\"{xmin},{ymin},{xmax},{ymax}\",\n",
    "                \"width\":    wp, \"height\": hp,\n",
    "                \"format\":   \"image/tiff\"\n",
    "            }\n",
    "            print(\"ğŸ”—\", f\"{base}?{urlencode(p, doseq=True)}\")\n",
    "            try:\n",
    "                r = requests.get(base, params=p, timeout=60); r.raise_for_status()\n",
    "                if \"image\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                    open(out, \"wb\").write(r.content)\n",
    "                    print(f\"âœ… {coverage} tile{idx} in {len(tiles)} tile(s)\")\n",
    "                    saved.append(out)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"WCS1.0 failed:\", e)\n",
    "\n",
    "        print(f\"{coverage} tile{idx} failed (v{version})\")\n",
    "\n",
    "    return saved\n",
    "\n",
    "def merge_tiles_with_gdalwarp(layer, tile_folder, output_folder):\n",
    "    tile_pattern = os.path.join(tile_folder, f\"{layer}*.tif\")\n",
    "    tile_paths = glob.glob(tile_pattern)\n",
    "\n",
    "    if not tile_paths:\n",
    "        raise FileNotFoundError(f\"No tiles found matching pattern: {tile_pattern}\")\n",
    "\n",
    "    print(f\"âœ… Found {len(tile_paths)} tiles to merge using gdal.Warp()...\")\n",
    "\n",
    "    output_path = os.path.join(output_folder, f\"{layer}.tif\")\n",
    "\n",
    "    # Debug: check first tile\n",
    "    test_ds = gdal.Open(tile_paths[0])\n",
    "    if test_ds is None:\n",
    "        raise RuntimeError(f\"Could not open sample tile: {tile_paths[0]}\")\n",
    "\n",
    "    if not output_path.lower().endswith(\".tif\"):\n",
    "        raise ValueError(f\"Output path must be a .tif file, got: {output_path}\")\n",
    "\n",
    "    print(f\"Merging into: {output_path}\")\n",
    "\n",
    "    # Merge tiles\n",
    "    vrt = gdal.Warp(output_path, tile_paths, format='GTiff', options=['COMPRESS=DEFLATE'])\n",
    "    if vrt is None:\n",
    "        raise RuntimeError(\"GDAL merge failed. Check input files and output path.\")\n",
    "    vrt = None  # Close dataset\n",
    "\n",
    "    print(f\"âœ… Merge completed: {output_path}\")\n",
    "\n",
    "    # Delete the original tiles after merging\n",
    "    for tile in tile_paths:\n",
    "        try:\n",
    "            os.remove(tile)\n",
    "            #print(f\"Deleted tile: {tile}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not delete tile {tile}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580361a7",
   "metadata": {},
   "source": [
    "**Executing WCS data download workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER INPUTS ===\n",
    "\n",
    "df = pd.read_csv(\"checkpoint05_ngr_WCS_metadata.csv\")\n",
    "#df['download']='yes'\n",
    "\n",
    "outline_28992 = veluwe28992\n",
    "bbox_28992 = square_bounds_VEL\n",
    "\n",
    "native_resolution = True   \n",
    "tile_folder = \"wcs_tiles\"\n",
    "output_folder = \"wcs_outputs\"\n",
    "\n",
    "os.makedirs(tile_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3016a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0.  Prepare a list that will hold one log-entry per layer\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "log_records = []        # â†’ [{\"layer\":..., \"success\":..., \"tiles\":..., \"duration_s\":...}, ...]\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "\n",
    "for _, row in df[df[\"download\"] == \"yes\"].iterrows():\n",
    "    crs   = int(row[\"crs_epsg\"])\n",
    "    layer = row[\"layer\"]\n",
    "\n",
    "    print(f\"\\nğŸ“¦ Processing {layer} (EPSG:{crs})\")\n",
    "\n",
    "    t0 = time.perf_counter()          # â”€â”€ start timer\n",
    "    downloaded_tiles = []             # track paths of downloaded tiles\n",
    "\n",
    "    try:\n",
    "        if crs == 28992:\n",
    "            native_resx, native_resy = map(float, ast.literal_eval(row[\"spatial_resolution\"]))\n",
    "\n",
    "            if native_resolution:\n",
    "                resx, resy = native_resx, native_resy\n",
    "                tiles = split_square_tiles(outline_28992, bbox_28992, resx, resy)\n",
    "                print(f\"   â†’ Downloading in {len(tiles)} tile(s) at {resx:.6f}Ã—{resy:.6f} m/px\")\n",
    "                for idx, tb in enumerate(tqdm(tiles, desc=\"   ğŸ“¥ tiles\"), start=1):\n",
    "                    path = download_tile_28992(row, tb, resx, resy, idx)\n",
    "                    if path:\n",
    "                        downloaded_tiles.append(path)\n",
    "            else:\n",
    "                w = bbox_28992[2] - bbox_28992[0]\n",
    "                h = bbox_28992[3] - bbox_28992[1]\n",
    "                resx, resy = w / 4000, h / 4000\n",
    "                print(f\"   â†’ Downloading 1 tile at {resx:.6f}Ã—{resy:.6f} m/px\")\n",
    "                path = download_tile_28992(row, bbox_28992, resx, resy, 1)\n",
    "                if path:\n",
    "                    downloaded_tiles.append(path)\n",
    "\n",
    "        elif crs in (4258, 4326):\n",
    "            print(f\"   â†’ Downloading geographic coverage with native_resolution={native_resolution}\")\n",
    "\n",
    "            downloaded_tiles = download_tile_degrees(\n",
    "                row, bbox_28992, native_resolution, tile_folder, output_folder\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(f\"âŒ Unsupported CRS EPSG:{crs}, skipping.\")\n",
    "            success = False\n",
    "            num_tiles = 0\n",
    "            raise RuntimeError(\"Unsupported CRS\")\n",
    "\n",
    "        # â”€â”€ Merge if we got at least one tile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if downloaded_tiles:\n",
    "            merge_tiles_with_gdalwarp(layer, tile_folder, output_folder)\n",
    "        else:\n",
    "            print(f\"âš ï¸ No tiles downloaded for {layer}, skipping merge.\")\n",
    "\n",
    "        success   = bool(downloaded_tiles)\n",
    "        num_tiles = len(downloaded_tiles)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ Error while processing {layer}: {e}\")\n",
    "        success   = False\n",
    "        num_tiles = 0\n",
    "\n",
    "    # â”€â”€ stop timer & log result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    duration = round(time.perf_counter() - t0, 2)     # seconds with 0.01 precision\n",
    "    log_records.append(\n",
    "        {\"layer\": layer, \"success\": success, \"tiles\": num_tiles, \"duration_s\": duration}\n",
    "    )\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  Convert log to DataFrame and save\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "wcs_download_log = pd.DataFrame(log_records)\n",
    "csv_path = Path(output_folder) / \"wcs_download_log.csv\"\n",
    "wcs_download_log.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Written summary to: {csv_path}\")\n",
    "print(wcs_download_log.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307c266",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c62d4",
   "metadata": {},
   "source": [
    "### 1.3. Download WFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_wfs_url(full_url):\n",
    "    parsed = urlparse(full_url)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))\n",
    "\n",
    "def get_wfs_feature_count(wfs_base_url, layer_name, verbose=False):\n",
    "    wfs_base_url = get_base_wfs_url(wfs_base_url)\n",
    "\n",
    "    def build_url(version):\n",
    "        if version == \"2.0.0\":\n",
    "            params = {\n",
    "                'service': 'WFS',\n",
    "                'version': version,\n",
    "                'request': 'GetFeature',\n",
    "                'typenames': layer_name,\n",
    "                'resultType': 'hits'\n",
    "            }\n",
    "        else:  # version 1.1.0\n",
    "            params = {\n",
    "                'service': 'WFS',\n",
    "                'version': version,\n",
    "                'request': 'GetFeature',\n",
    "                'typeName': layer_name,\n",
    "                'resultType': 'hits'\n",
    "            }\n",
    "        return f\"{wfs_base_url}?{urlencode(params)}\"\n",
    "\n",
    "    for version in [\"1.1.0\", \"2.0.0\"]:\n",
    "        url = build_url(version)\n",
    "        for attempt in range(1, 6):\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(f\"Attempt {attempt} with WFS {version}...\")\n",
    "                r = requests.get(url, timeout=30)\n",
    "                r.raise_for_status()\n",
    "                tree = etree.fromstring(r.content)\n",
    "\n",
    "                if version == \"2.0.0\":\n",
    "                    count = tree.attrib.get(\"numberMatched\")\n",
    "                else:\n",
    "                    count = tree.attrib.get(\"numberOfFeatures\")\n",
    "\n",
    "                return int(count) if count is not None else 0\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Attempt {attempt} failed with WFS {version}: {e}\")\n",
    "                if attempt < 5:\n",
    "                    time.sleep(2)\n",
    "\n",
    "    return None\n",
    "\n",
    "# === DOWNLOAD FUNCTION ===\n",
    "\n",
    "def download_full_wfs_layer_as_geodataframe(\n",
    "    wfs_base_url,\n",
    "    layer_name,\n",
    "    bbox=None,\n",
    "    bbox_crs=None,\n",
    "    max_features_per_request=2000,\n",
    "    get_feature_count_func=None,\n",
    "    verbose=False\n",
    "):\n",
    "    if get_feature_count_func is None:\n",
    "        raise ValueError(\"You must provide get_feature_count_func to retrieve total number of features.\")\n",
    "\n",
    "    total_features = get_feature_count_func(wfs_base_url, layer_name, verbose=verbose)\n",
    "    if total_features is None:\n",
    "        raise RuntimeError(\"Could not retrieve total feature count. Aborting.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total features to download: {total_features}\")\n",
    "\n",
    "    batch_gdfs = []\n",
    "\n",
    "    for start_index in range(0, total_features, max_features_per_request):\n",
    "        if verbose:\n",
    "            print(f\"Downloading features {start_index} to {start_index + max_features_per_request}...\")\n",
    "\n",
    "        gdf_batch = download_wfs_layer_as_geodataframe(\n",
    "            wfs_base_url=wfs_base_url,\n",
    "            layer_name=layer_name,\n",
    "            bbox=bbox,\n",
    "            bbox_crs=bbox_crs,\n",
    "            max_features=max_features_per_request,\n",
    "            start_index=start_index\n",
    "        )\n",
    "\n",
    "        if gdf_batch is None or len(gdf_batch) == 0:\n",
    "            if verbose:\n",
    "                print(\"No more features returned, stopping.\")\n",
    "            break\n",
    "\n",
    "        batch_gdfs.append(gdf_batch)\n",
    "\n",
    "    if len(batch_gdfs) == 0:\n",
    "        if verbose:\n",
    "            print(\"No features downloaded.\")\n",
    "        return None\n",
    "\n",
    "    gdf_full = pd.concat(batch_gdfs, ignore_index=True)\n",
    "    if verbose:\n",
    "        print(f\"Downloaded total {len(gdf_full)} features.\")\n",
    "\n",
    "    return gdf_full\n",
    "\n",
    "\n",
    "def download_wfs_layer_as_geodataframe(\n",
    "    wfs_base_url, \n",
    "    layer_name, \n",
    "    bbox=None, \n",
    "    bbox_crs='EPSG:28992', \n",
    "    max_features=1000, \n",
    "    start_index=0,\n",
    "    verbose=False\n",
    "):\n",
    "    import tempfile\n",
    "    from urllib.parse import urlencode\n",
    "\n",
    "    def build_params(version):\n",
    "        if version == '2.0.0':\n",
    "            key_layer = 'typenames'\n",
    "            key_count = 'count'\n",
    "        else:\n",
    "            key_layer = 'typeName'\n",
    "            key_count = 'maxFeatures'\n",
    "\n",
    "        params = {\n",
    "            'service': 'WFS',\n",
    "            'version': version,\n",
    "            'request': 'GetFeature',\n",
    "            key_layer: layer_name,\n",
    "            'outputFormat': 'application/json',\n",
    "            key_count: max_features,\n",
    "            'startIndex': start_index\n",
    "        }\n",
    "\n",
    "        if bbox is not None:\n",
    "            bbox_str = ','.join(map(str, bbox)) + f',{bbox_crs}'\n",
    "            params['bbox'] = bbox_str\n",
    "\n",
    "        return params\n",
    "\n",
    "    for version in ['1.1.0', '2.0.0']:\n",
    "        params = build_params(version)\n",
    "        url = f\"{wfs_base_url}?{urlencode(params)}\"\n",
    "\n",
    "        print(f\"\\nğŸŒ Trying WFS version {version}\")\n",
    "        print(f\"ğŸ”— URL: {url}\")\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            print(f\"ğŸ“¥ HTTP status: {r.status_code}\")\n",
    "            print(f\"ğŸ“„ First 300 characters of response:\\n{r.text[:300]}\")\n",
    "\n",
    "            r.raise_for_status()\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".geojson\", delete=False) as tmpfile:\n",
    "                tmp_filename = tmpfile.name\n",
    "                tmpfile.write(r.content)\n",
    "\n",
    "            gdf = gpd.read_file(tmp_filename)\n",
    "            os.remove(tmp_filename)\n",
    "\n",
    "            if gdf is not None and len(gdf) > 0:\n",
    "                print(f\"âœ… Retrieved {len(gdf)} features using WFS {version}\")\n",
    "                return gdf\n",
    "            else:\n",
    "                print(f\"âš ï¸ No data returned for version {version}. Trying fallback...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with WFS version {version}: {e}\")\n",
    "\n",
    "    print(\"âŒ All versions failed or returned no features.\")\n",
    "    return None\n",
    "\n",
    "def sanitize_datetime_columns(gdf):\n",
    "    \"\"\"\n",
    "    Converts all datetime-like columns in a GeoDataFrame to ISO-formatted strings.\n",
    "\n",
    "    This is necessary because some backends (e.g. Fiona/GeoJSON writer) \n",
    "    do not support pandas Timestamp or timezone-aware datetime objects.\n",
    "    \"\"\"\n",
    "    for col in gdf.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(gdf[col]) or isinstance(gdf[col].dtype, pd.DatetimeTZDtype):\n",
    "            gdf[col] = gdf[col].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "        elif gdf[col].dtype == 'object':\n",
    "            if gdf[col].apply(lambda x: isinstance(x, pd.Timestamp)).any():\n",
    "                gdf[col] = gdf[col].apply(\n",
    "                    lambda x: x.tz_localize(None).strftime('%Y-%m-%dT%H:%M:%S') if isinstance(x, pd.Timestamp) else x\n",
    "                )\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER INPUTS ===\n",
    "\n",
    "df = pd.read_csv(\"checkpoint05_ngr_WFS_metadata.csv\")\n",
    "df['download']='yes'\n",
    "bbox_28992 = [156913.384, 439295.44, 221547.716, 503929.772]\n",
    "output_folder = \"wfs_outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"download\"]==\"yes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17173082",
   "metadata": {},
   "source": [
    "**Executing WFS data download workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1.  Prepare a list that will collect one record per row processed\n",
    "# ------------------------------------------------------------------\n",
    "download_log = []  # each element: {\"layer\": ..., \"status\": ..., \"duration_s\": ...}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Main download loop with retry and timing\n",
    "# ------------------------------------------------------------------\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading WFS layers\"):\n",
    "    layer_name = row.get(\"layer\", \"unknown_layer\").strip()\n",
    "    wanted = str(row.get(\"download\", \"\")).strip().lower() == \"yes\"\n",
    "\n",
    "    if not wanted:\n",
    "        download_log.append({\"layer\": layer_name, \"status\": \"skipped\", \"duration_s\": 0})\n",
    "        continue\n",
    "\n",
    "    status = \"failed\"\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    for attempt in range(1, 4):  # Try up to 3 times\n",
    "        print(f\"\\nğŸ” Attempt {attempt}/3 for layer: {layer_name}\")\n",
    "        try:\n",
    "            wfs_base_url = get_base_wfs_url(row[\"wfs_getcapabilities_url\"])\n",
    "\n",
    "            gdf = download_full_wfs_layer_as_geodataframe(\n",
    "                wfs_base_url=wfs_base_url,\n",
    "                layer_name=layer_name,\n",
    "                bbox=bbox_28992,\n",
    "                bbox_crs=\"EPSG:28992\",\n",
    "                max_features_per_request=1000,\n",
    "                get_feature_count_func=get_wfs_feature_count,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            if gdf is not None:\n",
    "                if 'fid' in gdf.columns:\n",
    "                    print(f\"Dropping 'fid' column before saving: {layer_name}\")\n",
    "                    gdf = gdf.drop(columns='fid')\n",
    "\n",
    "                gdf = sanitize_datetime_columns(gdf)\n",
    "\n",
    "                output_path = os.path.join(output_folder, f\"{layer_name.replace(':', '_')}.geojson\")\n",
    "\n",
    "                datetime_cols = [\n",
    "                    col for col in gdf.columns\n",
    "                    if pd.api.types.is_datetime64_any_dtype(gdf[col]) or isinstance(gdf[col].dtype, pd.DatetimeTZDtype)\n",
    "                ]\n",
    "                if datetime_cols:\n",
    "                    print(f\"âš ï¸ Unconverted datetime columns before saving {layer_name}: {datetime_cols}\")\n",
    "\n",
    "                gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "                status = \"success\"\n",
    "                break  # Exit the retry loop if successful\n",
    "            else:\n",
    "                status = \"empty-result\"\n",
    "\n",
    "        except Exception as e:\n",
    "            status = f\"failed: {e}\"\n",
    "            print(f\"âŒ Attempt {attempt} failed for layer '{layer_name}': {e}\")\n",
    "            time.sleep(2)  # Wait before next attempt\n",
    "\n",
    "    duration = time.perf_counter() - total_start\n",
    "    download_log.append({\"layer\": layer_name, \"status\": status, \"duration_s\": round(duration, 2)})\n",
    "    time.sleep(2)  # Wait before moving to the next layer\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Write the log to CSV\n",
    "# ------------------------------------------------------------------\n",
    "log_df = pd.DataFrame(download_log)\n",
    "log_path = os.path.join(output_folder, \"download_summary.csv\")\n",
    "log_df.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ”ï¸  Download summary written to: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4577015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5035c21",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
